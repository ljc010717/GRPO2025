{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random #用于生成随机数\n",
    "import copy #用于深拷贝对象\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import wandb\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence #用于填充序列以匹配长度\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer #Hugging Face Transformers API，用于加载预训练语言模型和分词器\n",
    "from datasets import load_dataset #Hugging Face Datasets API，用于加载数据集\n",
    "\n",
    "def set_random_seed(seed: int = 42):\n",
    "    random.seed(seed) #设置Python内置随机数生成器的种子\n",
    "    torch.manual_seed(seed)#设置PyTorch的CPU随机数生成器种子\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)#设置所有GPU设备的随机数生成器种子\n",
    "    torch.backends.cudnn.deterministic = True#启用确定性模式，确保cuDNN操作结果一致\n",
    "    torch.backends.cudnn.benchmark = False#禁用自动优化算法选择\n",
    "\n",
    "set_random_seed(42)\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = \"\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义系统提示模板，要求模型输出包含<reasoning>和<answer>标签的内容\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "#从模型生成的文本中提取<answer>标签内的内容\n",
    "def extract_answer_from_model_output(text):\n",
    "   parts = text.split(\"<answer>\")#将文本按<answer>分割\n",
    "   if len(parts) < 2:  # No <answer> tag found\n",
    "       return None\n",
    "   last_part = parts[-1] #提取最后一个<answer>标签后的内容\n",
    "   if \"</answer>\" not in last_part:\n",
    "       return None\n",
    "   answer = last_part.split(\"</answer>\")[0].strip() #提取<answer>和</answer>之间的内容\n",
    "   return None if answer == \"...\" else answer\n",
    "\n",
    "#从数据集中提取答案部分（以####分隔）\n",
    "def extract_answer_from_dataset(text):\n",
    "   \n",
    "   # 检查文本是否包含'####'分隔符，该分隔符用于将问题与答案分开\n",
    "   if \"####\" not in text:\n",
    "       return None\n",
    "   # 如果找到分隔符，则将文本在此分隔符处分割，并返回第二部分（答案）\n",
    "   return text.split(\"####\")[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "def prepare_dataset(split=\"train\"):\n",
    "   data = load_dataset('gsm8k', 'main')[split]\n",
    "   formatted_data = []\n",
    "   for example in data:\n",
    "       prompt_str = build_prompt([\n",
    "           {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "           {\"role\": \"user\", \"content\": example[\"question\"]}\n",
    "       ]) #使用build_prompt函数生成提示字符串\n",
    "       formatted_example = {\n",
    "           \"prompt\": prompt_str,  # Now a string rather than a list.\n",
    "           \"answer\": extract_answer_from_dataset(example[\"answer\"])\n",
    "       }\n",
    "       formatted_data.append(formatted_example)\n",
    "   return formatted_data\n",
    "\n",
    "def build_prompt(messages):\n",
    "   #消息列表转换为单个字符串提示\n",
    "   return \"\\n\".join([msg[\"content\"].strip() for msg in messages])\n",
    "\n",
    "def extract_last_number(text):\n",
    "   #提取文本中最后出现的数字。\n",
    "   text = text.replace('$', '').replace('%', '')\n",
    "   #(?:^|\\s|=) 匹配文本的开头、空白字符或等号 =\n",
    "   #\\s* 匹配零个或多个空白字符\n",
    "   #(-?\\d*\\.?\\d+) 匹配一个完整的数字（可以是整数或小数，支持负数）\n",
    "   #\\s*$ 确保匹配的数字出现在文本的末尾\n",
    "   pattern = r'(?:^|\\s|=)\\s*(-?\\d*\\.?\\d+)\\s*$'\n",
    "   match = re.search(pattern, text)\n",
    "   #group(1) 提取正则表达式中第一个捕获组（即 (-?\\d*\\.?\\d+)）的内容\n",
    "   #将提取到的数字字符串转换为浮点数\n",
    "   return float(match.group(1)) if match else None\n",
    "\n",
    "def extract_single_number(text):\n",
    "   #如果文本中只有一个数字，提取该数字\n",
    "   #re.findall 是 Python 的正则表达式模块中的一个函数，用于查找所有匹配指定模式的子字符串\n",
    "   #匹配一个完整的数字（可以是整数或小数，支持负数）\n",
    "   numbers = re.findall(r'-?\\d*\\.?\\d+', text)\n",
    "   return float(numbers[0]) if len(numbers) == 1 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, eval_examples, device):\n",
    "   model.eval()\n",
    "   correct = 0\n",
    "   total = len(eval_examples)\n",
    "   print(\"\\n\" + \"=\"*50)\n",
    "   print(\"EVALUATION ON\", total, \"EXAMPLES\")\n",
    "   print(\"=\"*50)\n",
    "   for example in eval_examples:\n",
    "       full_prompt = example[\"prompt\"]\n",
    "       expected = example[\"answer\"]\n",
    "       #使用 tokenizer.encode 将提示文本编码为模型输入张量，并将其移动到指定设备。\n",
    "       inputs = tokenizer.encode(full_prompt, return_tensors=\"pt\").to(device)\n",
    "       #在 torch.no_grad() 上下文中运行模型推理，确保不计算梯度\n",
    "       with torch.no_grad():\n",
    "           outputs = model.generate(\n",
    "               inputs,\n",
    "               max_new_tokens=512,\n",
    "               temperature=0.7,\n",
    "               num_return_sequences=1,\n",
    "               pad_token_id=tokenizer.pad_token_id,\n",
    "               eos_token_id=tokenizer.eos_token_id,\n",
    "               forced_eos_token_id=tokenizer.eos_token_id,\n",
    "               early_stopping=False,\n",
    "           )\n",
    "        #使用 tokenizer.decode 将生成的 token 序列解码为字符串格式的响应\n",
    "       response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "       try:\n",
    "           predicted = extract_answer_from_model_output(response)\n",
    "           if predicted == expected: \n",
    "               is_correct = True\n",
    "           else:\n",
    "               # Try single number matching\n",
    "               pred_num = extract_single_number(str(predicted))\n",
    "               exp_num = extract_single_number(str(expected))\n",
    "               if pred_num is not None and exp_num is not None and pred_num == exp_num:\n",
    "                   is_correct = True\n",
    "               else:\n",
    "                   # Try last number matching\n",
    "                   pred_num = extract_last_number(str(predicted))\n",
    "                   exp_num = extract_last_number(str(expected))\n",
    "                   is_correct = (pred_num is not None and exp_num is not None and\n",
    "                               pred_num == exp_num)\n",
    "           # Update counter for correct answers\n",
    "           if is_correct:\n",
    "               correct += 1\n",
    "           # Print evaluation details\n",
    "           print(\"\\nPrompt:\")\n",
    "           print(full_prompt)\n",
    "           print(\"\\nExpected Answer:\")\n",
    "           print(expected)\n",
    "           print(\"\\nExtracted Answer:\")\n",
    "           print(predicted)\n",
    "           print(\"\\nFull Generated Response:\")\n",
    "           print(response)\n",
    "           print(\"\\nCorrect:\", \"✓\" if is_correct else \"✗\")\n",
    "           print(\"-\"*50)\n",
    "       except Exception as e:\n",
    "           print(\"\\nFailed to parse model output for prompt:\")\n",
    "           print(full_prompt)\n",
    "           print(\"Error:\", e)\n",
    "           print(\"-\"*50)\n",
    "   accuracy = (correct / total) * 100\n",
    "   print(f\"\\nAccuracy: {accuracy:.2f}% ({correct}/{total})\")\n",
    "   print(\"=\"*50)\n",
    "   #将模型恢复为训练模式（model.train()），以便后续继续训练\n",
    "   model.train()\n",
    "   return accuracy\n",
    "\n",
    "#精确匹配\n",
    "def correctness_reward(prompts, completions, answer, **kwargs): \n",
    "   #从 completions 中提取每个补全的文本内容\n",
    "   responses = [completion[0]['content'] for completion in completions]\n",
    "   extracted = [extract_answer_from_model_output(r) for r in responses]\n",
    "   rewards = []\n",
    "   for r, a in zip(extracted, answer):\n",
    "       print(\"r:\",r)\n",
    "       print(\"a:\",a)\n",
    "       if r == a:  # Exact match case\n",
    "           rewards.append(2.0)\n",
    "       else:\n",
    "           r_num = extract_single_number(str(r))\n",
    "           a_num = extract_single_number(str(a))\n",
    "           #如果两者都成功提取了数字，并且数字相等，认为是数值等价\n",
    "           if r_num is not None and a_num is not None and r_num == a_num:\n",
    "               rewards.append(1.5)\n",
    "           else:\n",
    "               rewards.append(0.0)\n",
    "   #对每个补全文本调用 .split() 方法，将其按空格分割为单词列表。计算单词列表的长度，表示补全的长度\n",
    "   completion_lengths = [len(response.split()) for response in responses]\n",
    "   return rewards\n",
    "\n",
    "#据模型生成内容是否符合指定的XML格式分配奖励分数\n",
    "def format_reward(completions, **kwargs):\n",
    "   responses = [completion[0]['content'] for completion in completions]\n",
    "   rewards = []\n",
    "   format_scores = []\n",
    "   for response in responses:\n",
    "       score = 0.0\n",
    "       if \"<reasoning>\" in response: score += 0.2\n",
    "       if \"</reasoning>\" in response: score += 0.2\n",
    "       if \"<answer>\" in response: score += 0.2\n",
    "       if \"</answer>\" in response: score += 0.2\n",
    "       rewards.append(score)\n",
    "       format_scores.append(score)\n",
    "   return rewards\n",
    "\n",
    "#将正确性奖励和格式奖励结合，生成综合奖励分数\n",
    "def combined_reward(prompts, completions, answer):\n",
    "   # Get individual rewards\n",
    "   correctness_scores = correctness_reward(prompts=prompts, completions=completions, answer=answer)\n",
    "   format_scores = format_reward(completions=completions)\n",
    "   # Combine rewards - correctness is weighted more heavily\n",
    "   combined_rewards = []\n",
    "   for c_score, f_score in zip(correctness_scores, format_scores):\n",
    "       # Correctness score range: 0.0 to 2.0\n",
    "       # Format score range: 0.0 to 0.8\n",
    "       # Total range: 0.0 to 2.8\n",
    "       combined_rewards.append(c_score + f_score)\n",
    "   return combined_rewards\n",
    "\n",
    "#计算特定 token 的 log 概率\n",
    "def selective_log_softmax(logits, input_ids):\n",
    "    #logits（torch.Tensor）：模型输出的原始 logits，形状通常为 [batch_size, sequence_length, vocab_size]\n",
    "    #input_ids（torch.Tensor）：需要计算 log 概率的 token ID，形状通常为 [batch_size, sequence_length]\n",
    "    \n",
    "    #dim=-1 表示在最后一个维度（词汇表维度）上进行操作\n",
    "    #结果是一个形状与 logits 相同的张量，表示每个 token 在词汇表上的 log 概率\n",
    "    #将 input_ids 的形状从 [batch_size, sequence_length] 扩展为 [batch_size, sequence_length, 1]\n",
    "    #使用 gather 操作从 log_probs 中提取指定位置的值\n",
    "    #去掉最后一个多余的维度，将结果形状恢复为 [batch_size, sequence_length]\n",
    "    log_probs = nn.functional.log_softmax(logits, dim=-1)\n",
    "    return log_probs.gather(dim=-1, index=input_ids.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "#计算一批 token 的 log 概率\n",
    "def compute_log_probs(model, input_ids, attention_mask, logits_to_keep):\n",
    "    #logits_to_keep（int）：从序列末尾保留的 token 数量\n",
    "    \n",
    "    #logits[:, :-1, :]选择所有 token 的 logits，除了最后一个 token\n",
    "    #在语言建模任务中，目标是预测下一个 token，因此每个位置的 logits 对应于下一个 token 的概率分布\n",
    "    #结果形状：[batch_size, sequence_length - 1, vocab_size]\n",
    "    logits = model(input_ids=input_ids, attention_mask=attention_mask).logits[:, :-1, :]\n",
    "    #从 input_ids 和 logits 中截取最后 logits_to_keep 个 token\n",
    "    #结果形状：[batch_size, logits_to_keep]\n",
    "    input_ids = input_ids[:, -logits_to_keep:]\n",
    "    #结果形状：[batch_size, logits_to_keep, vocab_size]\n",
    "    logits = logits[:, -logits_to_keep:, :]\n",
    "    return selective_log_softmax(logits, input_ids)\n",
    "\n",
    "#为生成的补全文本创建一个掩码，忽略序列中 EOS（End-of-Sequence）标记之后的所有 token\n",
    "def create_completion_mask(completion_ids, eos_token_id): \n",
    "    #对 completion_ids 中的每个元素进行比较，判断是否等于 eos_token_id\n",
    "    #返回一个布尔张量 is_eos，形状与 completion_ids 相同，其中值为 True 表示该位置是 EOS 标记\n",
    "    is_eos = completion_ids == eos_token_id\n",
    "    #创建一个形状为 [batch_size] 的张量，初始值为 is_eos.size(1)（即序列的最大长度）\n",
    "    #数据类型为 torch.long，设备与 completion_ids 一致\n",
    "    eos_idx = torch.full((is_eos.size(0),), is_eos.size(1), dtype=torch.long, device=completion_ids.device)\n",
    "    #按行（dim=1）检查 is_eos 中是否存在至少一个 True 值\n",
    "    mask_exists = is_eos.any(dim=1)\n",
    "    #将布尔张量 is_eos 转换为整数张量\n",
    "    #按行（dim=1）找到第一个最大值（即第一个 1）的索引\n",
    "    #仅对包含 EOS 标记的序列更新 eos_idx\n",
    "    eos_idx[mask_exists] = is_eos.int().argmax(dim=1)[mask_exists]\n",
    "    #创建一个从 0 到 sequence_length - 1 的范围张量\n",
    "    #将范围张量扩展为形状 [batch_size, sequence_length]，以便与 completion_ids 匹配\n",
    "    sequence_indices = torch.arange(is_eos.size(1), device=completion_ids.device).expand(is_eos.size(0), -1)\n",
    "    #将 eos_idx 的形状从 [batch_size] 扩展为 [batch_size, 1]，以便与 sequence_indices 进行广播操作\n",
    "    #比较每个 token 的索引是否小于或等于第一个 EOS 标记的索引\n",
    "    #将布尔张量转换为整数张量\n",
    "    return (sequence_indices <= eos_idx.unsqueeze(1)).int()\n",
    "\n",
    "def generate_completions(model, tokenizer, prompts, num_generations=4, max_completion_length=32):   \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #使用分词器将 prompts 编码为 PyTorch 张量\n",
    "    #padding=True：对序列进行填充，使其长度一致\n",
    "    #padding_side=\"left\"：在左侧填充，确保补全生成从提示的末尾开始\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, padding_side=\"left\")\n",
    "    prompt_ids = inputs[\"input_ids\"].to(device)\n",
    "    prompt_mask = inputs[\"attention_mask\"].to(device)\n",
    "    print(f\"Input batch size: {prompt_ids.size(0)}, Device before model: {prompt_ids.device}\")\n",
    "    #获取提示的序列长度\n",
    "    prompt_length = prompt_ids.size(1)\n",
    "    #在批次维度上重复每个提示 num_generations 次\n",
    "    prompt_ids = prompt_ids.repeat_interleave(num_generations, dim=0)\n",
    "    prompt_mask = prompt_mask.repeat_interleave(num_generations, dim=0)\n",
    "    outputs = model.generate(\n",
    "        prompt_ids,\n",
    "        attention_mask=prompt_mask,\n",
    "        max_new_tokens=max_completion_length,\n",
    "        do_sample=True,#启用采样生成（而非贪婪解码）\n",
    "        temperature=1.0,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        early_stopping=False\n",
    "    )\n",
    "    print(f\"Output batch size: {outputs.size(0)}, Device after model: {outputs.device}\")\n",
    "    #截取生成输出中的补全文本部分（排除提示部分）\n",
    "    completion_ids = outputs[:, prompt_length:]\n",
    "    #调用 create_completion_mask 函数，生成补全掩码，忽略 EOS 标记之后的 token\n",
    "    completion_mask = create_completion_mask(completion_ids, tokenizer.eos_token_id)\n",
    "    return prompt_ids, prompt_mask, completion_ids, completion_mask\n",
    "\n",
    "#生成用于 GRPO（Generalized Reinforcement Policy Optimization）训练所需的数据，包括补全文本、log 概率等\n",
    "def generate_rollout_data(model, ref_model, tokenizer, batch_samples, num_generations, max_completion_length):\n",
    "    #model：当前策略模型，用于生成补全和计算 log 概率\n",
    "    #ref_model：参考模型，用于计算 KL 散度\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #遍历 batch_samples，如果样本是字典，则提取 \"prompt\" 字段；否则提取第一个元素\n",
    "    prompts = [sample[\"prompt\"] if isinstance(sample, dict) else sample[0] for sample in batch_samples]\n",
    "    answers = [sample[\"answer\"] if isinstance(sample, dict) else sample[1] for sample in batch_samples]\n",
    "    with torch.no_grad():\n",
    "        prompt_ids, prompt_mask, completion_ids, completion_mask = generate_completions(\n",
    "            model, tokenizer, prompts, num_generations, max_completion_length\n",
    "        )\n",
    "        #在序列维度上拼接提示和补全的 token ID\n",
    "        input_ids = torch.cat([prompt_ids, completion_ids], dim=1)\n",
    "        attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)\n",
    "        #获取补全文本的长度（token 数量），用于指定需要保留的 logits 数量\n",
    "        logits_to_keep = completion_ids.size(1)\n",
    "        old_log_probs = compute_log_probs(model, input_ids, attention_mask, logits_to_keep)\n",
    "        ref_log_probs = compute_log_probs(ref_model, input_ids, attention_mask, logits_to_keep)\n",
    "    #遍历 completion_ids，对每个补全调用 tokenizer.decode 将其解码为字符串\n",
    "    formatted_completions = [[{'content': tokenizer.decode(ids, skip_special_tokens=True)}] for ids in completion_ids]\n",
    "    #对每个提示重复 num_generations 次\n",
    "    repeated_prompts = [p for p in prompts for _ in range(num_generations)]\n",
    "    repeated_answers = [a for a in answers for _ in range(num_generations)]\n",
    "    # input_ids：完整的输入 token ID（提示 + 补全）。\n",
    "    # attention_mask：完整的注意力掩码（提示 + 补全）。\n",
    "    # completion_mask：补全文本的掩码。\n",
    "    # old_log_probs：策略模型的 log 概率。\n",
    "    # ref_log_probs：参考模型的 log 概率。\n",
    "    # formatted_completions：格式化的补全文本。\n",
    "    # repeated_prompts：重复的提示。\n",
    "    # repeated_answers：重复的答案。\n",
    "    # logits_to_keep：需要保留的 logits 数量。\n",
    "    # batch_size：批次大小（提示数量）。\n",
    "    # num_generations：每个提示生成的补全数量\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"completion_mask\": completion_mask,\n",
    "        \"old_log_probs\": old_log_probs,\n",
    "        \"ref_log_probs\": ref_log_probs,\n",
    "        \"formatted_completions\": formatted_completions,\n",
    "        \"repeated_prompts\": repeated_prompts,\n",
    "        \"repeated_answers\": repeated_answers,\n",
    "        \"logits_to_keep\": logits_to_keep,\n",
    "        \"batch_size\": len(prompts),\n",
    "        \"num_generations\": num_generations\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算用于更新策略模型的 GRPO（Generalized Reinforcement Policy Optimization）损失\n",
    "def grpo_loss(model, ref_model, rollout_data, tokenizer, reward_function, beta=0.01, epsilon=0.2):\n",
    "    # model：当前策略模型，用于生成补全文本和计算 log 概率。\n",
    "    # ref_model：参考模型，用于计算 KL 散度。\n",
    "    # rollout_data（dict）：由 generate_rollout_data 函数生成的数据，包含输入 token ID、注意力掩码、补全掩码、旧的 log 概率等。\n",
    "    # tokenizer：分词器，用于对文本进行编码和解码。\n",
    "    # reward_function：奖励函数，用于计算每个补全的奖励分数。\n",
    "    # beta（float）：KL 散度惩罚系数，默认为 0.01。\n",
    "    # epsilon（float）：PPO 裁剪参数，默认为 0.2。  \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    input_ids = rollout_data[\"input_ids\"]\n",
    "    attention_mask = rollout_data[\"attention_mask\"]\n",
    "    completion_mask = rollout_data[\"completion_mask\"]\n",
    "    logits_to_keep = rollout_data[\"logits_to_keep\"]\n",
    "    old_log_probs = rollout_data[\"old_log_probs\"]\n",
    "    ref_log_probs = rollout_data[\"ref_log_probs\"]\n",
    "    #使用当前策略模型计算补全文本的 log 概率\n",
    "    token_log_probs = compute_log_probs(model, input_ids, attention_mask, logits_to_keep)\n",
    "    # token_log_probs - old_log_probs：计算新旧策略的 log 概率差。\n",
    "    # torch.exp(...)：将 log 概率差转换为概率比\n",
    "    ratio = torch.exp(token_log_probs - old_log_probs)\n",
    "    #输入提示、格式化的补全文本和答案，计算奖励分数\n",
    "    rewards = torch.tensor(\n",
    "        reward_function(prompts=rollout_data[\"repeated_prompts\"], completions=rollout_data[\"formatted_completions\"], answer=rollout_data[\"repeated_answers\"]),\n",
    "        dtype=torch.float32,\n",
    "        device=device\n",
    "    )\n",
    "    # print(f\"Rewards: {rewards}\")  # Debug rewards\n",
    "    batch_size = rollout_data[\"batch_size\"]\n",
    "    num_generations = rollout_data[\"num_generations\"]\n",
    "    #：将奖励分数重新组织为 [batch_size, num_generations] 的形状。\n",
    "    rewards = rewards.view(batch_size, num_generations)\n",
    "    # avg_reward：计算所有奖励的平均值，并打印出来。\n",
    "    # mean_rewards：计算每个提示的奖励均值，并重复以匹配补全数量。\n",
    "    # std_rewards：计算每个提示的奖励标准差，并重复以匹配补全数量。\n",
    "    # advantages：通过标准化公式计算优势函数，并调整形状\n",
    "    avg_reward = rewards.mean().item()\n",
    "    print(\"Average Reward:\", avg_reward)\n",
    "    mean_rewards = rewards.mean(dim=1).repeat_interleave(num_generations)\n",
    "    std_rewards = rewards.std(dim=1).repeat_interleave(num_generations)\n",
    "    advantages = ((rewards.view(-1) - mean_rewards) / (std_rewards + 1e-4)).unsqueeze(1)\n",
    "    #计算 PPO 替代目标\n",
    "    #计算未裁剪的目标\n",
    "    surr1 = ratio * advantages\n",
    "    #对概率比进行裁剪，限制在 [1 - epsilon, 1 + epsilon] 范围内\n",
    "    #计算裁剪后的目标\n",
    "    surr2 = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * advantages\n",
    "    surrogate_loss = torch.min(surr1, surr2)\n",
    "    #计算 KL 散度\n",
    "    kl = torch.exp(ref_log_probs - token_log_probs) - (ref_log_probs - token_log_probs) - 1\n",
    "    #组合替代损失和 KL 散度，得到每个 token 的损失\n",
    "    per_token_loss = surrogate_loss - beta * kl\n",
    "    #(per_token_loss * completion_mask).sum(dim=1)：按补全掩码加权求和\n",
    "    #/ completion_mask.sum(dim=1)：对每个补全的损失进行归一化\n",
    "    loss = -((per_token_loss * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)).mean()\n",
    "    return loss, avg_reward\n",
    "\n",
    "#使用 GRPO（Generalized Reinforcement Policy Optimization）方法对语言模型进行训练\n",
    "def train_with_grpo(model, tokenizer, train_data, num_iterations=1, num_steps=500, batch_size=4,\n",
    "                              num_generations=4, max_completion_length=128, beta=0.1,\n",
    "                              learning_rate=5e-6, mu=3, epsilon=0.2, reward_function=None, device_ids=None):\n",
    "    # beta（float）：KL 散度惩罚系数。\n",
    "    # learning_rate（float）：优化器的学习率。\n",
    "    # mu（int）：每个批次中的策略更新次数。\n",
    "    # epsilon（float）：PPO 裁剪参数。\n",
    "    # reward_function：奖励函数，用于计算补全的奖励分数\n",
    " \n",
    "    #确保代码在至少两个 GPU 上运行，并设置默认设备\n",
    "    assert device_ids is not None and len(device_ids) > 1, \"This code needs at least 2 GPU cores to run!\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #nn.DataParallel(model, device_ids=device_ids)：将模型分配到指定的 GPU 上\n",
    "    model = nn.DataParallel(model, device_ids=device_ids)\n",
    "    print(f\"Model wrapped with DataParallel across GPUs: {device_ids}\")\n",
    "    #执行多次外层迭代，每次迭代都会创建一个新的参考模型并重新初始化优化器\n",
    "    for iteration in range(num_iterations):\n",
    "        print(f\"\\nIteration {iteration+1}/{num_iterations}\")\n",
    "        #复制当前策略模型（model.module 是 DataParallel 包装的原始模型）为参考模型\n",
    "        ref_model = copy.deepcopy(model.module)\n",
    "        ref_model.eval()\n",
    "        for param in ref_model.parameters():\n",
    "            param.requires_grad = False #冻结参考模型的参数，避免梯度更新\n",
    "        print(\"Reference model created.\")\n",
    "        #为当前策略模型重新初始化优化器，并将其设置为训练模式\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        model.train()\n",
    "        #每个训练步骤中，随机采样一批数据并生成补全文本及其相关数据\n",
    "        for step in range(num_steps):\n",
    "            batch_samples = random.sample(train_data, batch_size)\n",
    "            with torch.no_grad():\n",
    "                rollout_data = generate_rollout_data(\n",
    "                    model.module,\n",
    "                    ref_model,\n",
    "                    tokenizer,\n",
    "                    batch_samples,\n",
    "                    num_generations,\n",
    "                    max_completion_length\n",
    "                )\n",
    "            for grpo_iter in range(mu):\n",
    "                loss, avg_reward = grpo_loss(\n",
    "                    model.module,\n",
    "                    ref_model,\n",
    "                    rollout_data,\n",
    "                    tokenizer,\n",
    "                    reward_function,\n",
    "                    beta=beta,\n",
    "                    epsilon=epsilon\n",
    "                )\n",
    "                # 调用 grpo_loss 函数，计算 GRPO 损失和平均奖励。\n",
    "                # optimizer.zero_grad()：清空梯度。\n",
    "                # loss.backward()：反向传播计算梯度。\n",
    "                # torch.nn.utils.clip_grad_norm_(...)：裁剪梯度，防止梯度爆炸。\n",
    "                # optimizer.step()：更新模型参数\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n",
    "                optimizer.step()\n",
    "                # Log to wandb\n",
    "                wandb.log({\n",
    "                    \"loss\": loss.item(),\n",
    "                    \"average_reward\": avg_reward,\n",
    "                    \"iteration\": iteration + 1,\n",
    "                    \"step\": step + 1,\n",
    "                    \"grpo_iter\": grpo_iter + 1\n",
    "                })\n",
    "                print(f\"Iteration {iteration+1}/{num_iterations}, Step {step+1}/{num_steps}, \"\n",
    "                      f\"GRPO iter {grpo_iter+1}/{mu}, loss: {loss.item():.4f}\")\n",
    "                #for i in range(torch.cuda.device_count()):\n",
    "                #    print(f\"GPU {i} Usage: {torch.cuda.memory_allocated(i) / 1024**2:.2f} MiB, \"\n",
    "                #          f\"Utilization: {torch.cuda.utilization(i)}%\")\n",
    "                # Uncomment to see the GPU utilization stats\n",
    "    return model.module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#优化模型以减少训练过程中的内存占用\n",
    "def optimize_model_memory(model): \n",
    "    model.train()\n",
    "    #禁用键值（KV）缓存以节省内存\n",
    "    # 在生成任务中，KV 缓存用于存储注意力机制中的中间结果以加速推理。但在训练过程中，禁用 KV 缓存可以显著减少内存占用。\n",
    "    model.config.use_cache = False\n",
    "    # First ensure inputs will require gradients\n",
    "    #确保输入嵌入层的输出需要梯度，以便在反向传播时计算梯度\n",
    "    #检查模型是否具有内置方法 enable_input_require_grads\n",
    "    if hasattr(model, \"enable_input_require_grads\"):\n",
    "        model.enable_input_require_grads()\n",
    "    else:\n",
    "        #定义一个前向钩子函数 make_inputs_require_grad，将输入嵌入层的输出标记为需要梯度\n",
    "        def make_inputs_require_grad(module, input, output):\n",
    "            output.requires_grad_(True)\n",
    "        #使用 register_forward_hook 将钩子函数注册到输入嵌入层\n",
    "        model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n",
    "    #启用梯度检查点（Gradient Checkpointing），以牺牲计算效率换取内存节省\n",
    "    #梯度检查点是一种内存优化技术，通过重新计算部分前向传播的结果来减少内存占用。\n",
    "    #这在训练大型语言模型时非常有用，尤其是在 GPU 内存有限的情况下\n",
    "    model.gradient_checkpointing_enable()\n",
    "    return model\n",
    "\n",
    "# Main execution\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using primary device: {device}\")\n",
    "\n",
    "model_name = \"Qwen2.5-3B-Instruct\"\n",
    "output_dir = \"math_solver_model\"\n",
    "print(\"Downloading model...\")\n",
    "# AutoModelForCausalLM.from_pretrained(...)：\n",
    "# 加载因果语言模型（Causal Language Model）。\n",
    "# torch_dtype=torch.bfloat16：使用 bfloat16 数据类型以节省显存。\n",
    "# device_map=\"auto\"：自动将模型分配到多个 GPU 或 CPU 上\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(\"Model downloaded\")\n",
    "\n",
    "\n",
    "#padding_side=\"left\"：设置填充方向为左侧，确保补全生成从提示的末尾开始\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "#tokenizer.pad_token = tokenizer.eos_token：将填充 token 设置为结束 token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Detected {num_gpus} GPUs\")\n",
    "device_ids = list(range(num_gpus)) if num_gpus > 1 else None\n",
    "all_data = prepare_dataset(\"train\")\n",
    "random.shuffle(all_data)\n",
    "size_of_eval_data = 30  # change to a smaller value to save time or to a larger number for a more reliable estimate\n",
    "eval_data = all_data[:size_of_eval_data]\n",
    "train_data = all_data[size_of_eval_data:]\n",
    "\n",
    "print(\"\\nInitial model evaluation before finetuning:\")\n",
    "pre_grpo_accuracy = evaluate_model(model, tokenizer, eval_data, device)\n",
    "print(f\"Pre-GRPO Accuracy: {pre_grpo_accuracy:.2f}%\")\n",
    "\n",
    "model = optimize_model_memory(model)\n",
    "\n",
    "print(\"\\nStarting RL fine-tuning using GRPO...\")\n",
    "# This config was tested on a 8xA100 node, where each A100 is has 80GB of VRAM\n",
    "training_config = {\n",
    "    'num_iterations': 1,\n",
    "    'num_steps': 500,\n",
    "    'batch_size': 7, # reduce if you have fewer GPUs\n",
    "    'num_generations': 8, # reduce if you have GPUs with less VRAM\n",
    "    'max_completion_length': 256, # reduce if you have GPUs with less VRAM\n",
    "    'beta': 0.04,\n",
    "    'learning_rate': 5e-6,\n",
    "    'mu': 1,\n",
    "    'epsilon': 0.1\n",
    "}\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "wandb.init(project=os.environ[\"WANDB_PROJECT\"], reinit=True)\n",
    "print(\"Weights & Biases initialized.\")\n",
    "\n",
    "model = train_with_grpo(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_data=train_data,\n",
    "    reward_function=combined_reward,\n",
    "    device_ids=device_ids,\n",
    "    **training_config\n",
    ")\n",
    "\n",
    "wandb.finish()\n",
    "print(\"Training completed and wandb run finished.\")\n",
    "\n",
    "print(\"\\nFinal model evaluation after GRPO RL fine-tuning:\")\n",
    "post_grpo_accuracy = evaluate_model(model, tokenizer, eval_data, device)\n",
    "print(f\"Post-GRPO Accuracy: {post_grpo_accuracy:.2f}%\")\n",
    "\n",
    "print(\"\\nSaving GRPO fine-tuned model...\")\n",
    "# model.save_pretrained(...)：保存微调后的模型权重。\n",
    "# tokenizer.save_pretrained(...)：保存分词器配置\n",
    "model.save_pretrained(\"grpo_finetuned_model\")\n",
    "tokenizer.save_pretrained(\"grpo_finetuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grpo03121810",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
